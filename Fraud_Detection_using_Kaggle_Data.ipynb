{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqnjSueJhFcMk+8kV701aL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbortey9/Big_Data-EDA-Regularization-/blob/main/Fraud_Detection_using_Kaggle_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4LtwP8rUX3f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold,cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import RFE\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve,confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import joblib\n",
        "from sklearn import metrics\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re  # Import the 're' module for regular expressions\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "gqenLbHnUavq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "H1ZEbcVrUezy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the data\n",
        "Data=pd.read_csv('/content/drive/MyDrive/Fraud/creditcard.csv')"
      ],
      "metadata": {
        "id": "xQqzeRoyUhH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "waD5s5-bFPC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.info()"
      ],
      "metadata": {
        "id": "4wUahxKSFh7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data = Data.drop([\"Amount\", \"Time\"], axis=1)"
      ],
      "metadata": {
        "id": "nm1RgNT4FxJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "RQrS4S7DG_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check and drop deplicate columns\n",
        "print('Before dedup:', Data.shape)\n",
        "Data_cln=Data.loc[:, ~Data.columns.duplicated()]\n",
        "print('After dedup:', Data_cln.shape)\n",
        "duplicateCols=Data.loc[:, Data.columns.duplicated()]\n",
        "if (duplicateCols.shape[1] !=0):\n",
        "    print('Number of duplicated columns dropped:', duplicatedCols.shape[1])\n",
        "    print(\"Dupliate columns except first occurrences:\")\n",
        "    print(list(duplicateCols.columns))"
      ],
      "metadata": {
        "id": "VX9GgEqfHEKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Missing and Zero rate\n",
        "def zero_missing_unique(df):\n",
        "    var_miss_rate=df.isnull().sum(axis=0)/df.shape[0]\n",
        "    var_miss_rate=var_miss_rate.to_frame('missing_rate')\n",
        "\n",
        "    var_zero_rate=(df==0).astype(int).sum(axis=0)/df.shape[0]\n",
        "    var_zero_rate=var_zero_rate.to_frame('zero_rate')\n",
        "    var_unique=df.nunique().to_frame('count_unique')\n",
        "    var_type=df.dtypes.to_frame('data_type')\n",
        "    data_stat=pd.concat([var_miss_rate, var_zero_rate,var_unique, var_type], axis=1)\n",
        "    return data_stat\n",
        "stat_zero_missing_unique=zero_missing_unique(Data)# just change the data name here to your own data\n",
        "stat_zero_missing_unique"
      ],
      "metadata": {
        "id": "DvM-pQyLHXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.describe().T"
      ],
      "metadata": {
        "id": "j40Un5bRprNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Scalar Transformation to the Time and Amount Variables"
      ],
      "metadata": {
        "id": "VwwcqBT_iHms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns you want to standardize\n",
        "columns_to_standardize = ['Time', 'Amount']\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply the scalar transformation to the selected columns\n",
        "Data[columns_to_standardize] = scaler.fit_transform(Data[columns_to_standardize])\n"
      ],
      "metadata": {
        "id": "IEzRA33Ceg1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dependent and independent indices\n",
        "X_data=Data.drop(columns=['Class'])\n",
        "Y_data=Data['Class']"
      ],
      "metadata": {
        "id": "kFMb0wgdhf7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the correlation between the independent Variables\n",
        "correlation_matrix=X_data.corr()\n",
        "# Assuming df is your DataFrame with independent variables\n",
        "correlation_matrix =X_data.corr()\n",
        "\n",
        "# Create a mask to hide the upper triangle (to avoid duplication)\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Define a custom colormap\n",
        "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "\n",
        "# Set the threshold for the color scale\n",
        "threshold = 0.95\n",
        "\n",
        "# Create a heatmap with the custom colormap and threshold\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=cmap, mask=mask, vmin=-threshold, vmax=threshold)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MrmIsXbnigk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.95\n",
        "correlated_pairs = set()\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname_i = correlation_matrix.columns[i]\n",
        "            colname_j = correlation_matrix.columns[j]\n",
        "            correlated_pairs.add((colname_i, colname_j))"
      ],
      "metadata": {
        "id": "o_vKrVvoinmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var1, var2 in correlated_pairs:\n",
        "    # Decide which variable to keep and remove the other\n",
        "    # For example, removing var2:\n",
        "    if var2 in X_data.columns:\n",
        "        X_data.drop(var2, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ivf6PK0yjR8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into a training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "M86iZHAWjUNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Apply SMOTE to the target variable only\n",
        "smote = SMOTE(sampling_strategy='auto',random_state=42)\n",
        "X_resampled,y_resampled = smote.fit_resample(X_train,y_train)"
      ],
      "metadata": {
        "id": "M9gIOC6UYHha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "# Resampled class distribution\n",
        "print(\"Class distribution after SMOTE (Target variable only):\", Counter(y_train))"
      ],
      "metadata": {
        "id": "ZomB3gHFaWlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame with the resampled target variable y_resampled\n",
        "Y_resampled= pd.DataFrame({\n",
        "    'y_resampled': y_resampled  # Replace with your actual resampled target variable\n",
        "})\n"
      ],
      "metadata": {
        "id": "KP1Tc1NtaXQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each class\n",
        "class_counts =Y_resampled['y_resampled'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_counts.plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution (After SMOTE)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o2u5kt5vaZR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=10)\n",
        "\n",
        "# Fit the Random Forest model on the training data\n",
        "clf.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Get feature importances\n",
        "importance_scores = clf.feature_importances_\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X_resampled.columns"
      ],
      "metadata": {
        "id": "L1N_7w8gaeZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store feature names and importance scores\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})\n",
        "\n",
        "# Sort the DataFrame by importance scores in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Select the top 20 features based on importance scores\n",
        "top_20_features_Random= feature_importance_df.head(20)"
      ],
      "metadata": {
        "id": "NdwcrH6xaqwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot feature importance scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(top_20_features_Random['Feature'], top_20_features_Random['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 20 Features with Highest Importance Scores')\n",
        "plt.gca().invert_yaxis()  # Invert the y-axis for better readability\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hh1mWZTRathM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_20_features_Random"
      ],
      "metadata": {
        "id": "fhPvSsuEpEe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the names of the top features from the DataFrame\n",
        "final_feature_names = top_20_features_Random['Feature'].tolist()\n",
        "# Create a new DataFrame with only the selected top features\n",
        "X_train = X_train[final_feature_names]\n",
        "X_test=X_test[final_feature_names]"
      ],
      "metadata": {
        "id": "i2ECg3YgtY5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_train and y_train to NumPy arrays\n",
        "X_train_np= np.array(X_train)\n",
        "y_train_np= np.array(y_train)"
      ],
      "metadata": {
        "id": "DDtA6MR8uGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tA0g60LOtghk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(kernel='rbf', probability=True)))\n",
        "#models.append(('SVM', SVC(kernel='rbf')))\n",
        "models.append(('RF', RandomForestClassifier()))\n",
        "models.append(('XGB', XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', nthread=1, eval_metric='rmse')))\n",
        "models.append(('LightGBM', lgb.LGBMClassifier()))"
      ],
      "metadata": {
        "id": "sXXFUe5Js1Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, roc_curve, auc, confusion_matrix, average_precision_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer  # Import make_scorer\n"
      ],
      "metadata": {
        "id": "rf57popbtyUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store results\n",
        "results_train_cv = {'Model': [], 'KS_Train_CV': [], 'AUC_Train_CV': [], 'F1-Score_Train_CV': [], 'Recall_Train_CV': [], 'PRAUC_Train_CV': [],'Precision_Train_CV': []}"
      ],
      "metadata": {
        "id": "r8cuNvEmtz4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom scoring function for KS\n",
        "def custom_ks_scorer(y_true, y_pred_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "    return max(tpr - fpr)\n"
      ],
      "metadata": {
        "id": "tKGmpNxMt2Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a custom scoring function for KS\n",
        "def custom_ks_scorer(y_true, y_pred_prob):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "    return max(tpr - fpr)\n",
        "\n"
      ],
      "metadata": {
        "id": "zg90H_G0t3uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define K-Fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)  # You can adjust the number of splits and other parameters"
      ],
      "metadata": {
        "id": "JAPCUzVot7cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through models\n",
        "for name, model in models:\n",
        "    # Initialize lists to store scores for each fold\n",
        "    ks_scores = []\n",
        "    auc_scores = []\n",
        "    prauc_scores = []  # Added for PRAUC\n",
        "    f1_scores = []\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "\n",
        "    # Perform K-Fold cross-validation\n",
        "    for train_index, val_index in kf.split(X_train_np):  # Use X_train_np\n",
        "        X_train_fold, X_val_fold = X_train_np[train_index], X_train_np[val_index]\n",
        "        y_train_fold, y_val_fold = y_train_np[train_index], y_train_np[val_index]  # Use y_train_np\n",
        "\n",
        "        # Fit the model on the training fold\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Predict probabilities on the validation fold\n",
        "        y_val_pred_prob = model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "        # Calculate KS, AUC, PRAUC, F1-Score, Recall, and Precision for the fold\n",
        "        ks_fold = custom_ks_scorer(y_val_fold, y_val_pred_prob)\n",
        "        auc_fold = roc_auc_score(y_val_fold, y_val_pred_prob)\n",
        "        prauc_fold = average_precision_score(y_val_fold, y_val_pred_prob)  # Calculate PRAUC\n",
        "        f1_fold = f1_score(y_val_fold, (y_val_pred_prob > 0.5).astype(int))\n",
        "        recall_fold = recall_score(y_val_fold, (y_val_pred_prob > 0.5).astype(int))\n",
        "        precision_fold = precision_score(y_val_fold, (y_val_pred_prob > 0.5).astype(int))\n",
        "\n",
        "        ks_scores.append(ks_fold)\n",
        "        auc_scores.append(auc_fold)\n",
        "        prauc_scores.append(prauc_fold)  # Store PRAUC\n",
        "        f1_scores.append(f1_fold)\n",
        "        recall_scores.append(recall_fold)\n",
        "        precision_scores.append(precision_fold)\n",
        "\n",
        "    # Store the mean scores in the results dictionary\n",
        "    results_train_cv['Model'].append(name)\n",
        "    results_train_cv['KS_Train_CV'].append(np.mean(ks_scores))\n",
        "    results_train_cv['AUC_Train_CV'].append(np.mean(auc_scores))\n",
        "    results_train_cv['PRAUC_Train_CV'].append(np.mean(prauc_scores))  # Store PRAUC\n",
        "    results_train_cv['F1-Score_Train_CV'].append(np.mean(f1_scores))\n",
        "    results_train_cv['Recall_Train_CV'].append(np.mean(recall_scores))\n",
        "    results_train_cv['Precision_Train_CV'].append(np.mean(precision_scores))"
      ],
      "metadata": {
        "id": "VA4ZwDKut9wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for training set K-Fold cross-validation results\n",
        "df_results_train_cv = pd.DataFrame(results_train_cv)"
      ],
      "metadata": {
        "id": "vxj3yh6nuLfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the DataFrame\n",
        "print(\"K-Fold Cross-Validation Results on Training Set:\")\n",
        "print(df_results_train_cv)\n",
        "\n",
        "df_results_train_cv.to_excel('/content/drive/MyDrive/GSS_Data/Fraud_train.xlsx')"
      ],
      "metadata": {
        "id": "omYWwjSZuNeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best model based on the highest mean KS score\n",
        "best_model_ks = df_results_train_cv.loc[df_results_train_cv['KS_Train_CV'].idxmax()]['Model']\n",
        "\n",
        "# Find the best model based on the highest mean AUC score\n",
        "best_model_auc = df_results_train_cv.loc[df_results_train_cv['AUC_Train_CV'].idxmax()]['Model']\n",
        "\n",
        "# Find the best model based on the highest mean F1-Score\n",
        "best_model_f1 = df_results_train_cv.loc[df_results_train_cv['F1-Score_Train_CV'].idxmax()]['Model']\n",
        "\n",
        "# Find the best model based on the highest mean Recall score\n",
        "best_model_recall = df_results_train_cv.loc[df_results_train_cv['Recall_Train_CV'].idxmax()]['Model']\n",
        "\n",
        "# Find the best model based on the highest mean Precision score\n",
        "best_model_precision = df_results_train_cv.loc[df_results_train_cv['PRAUC_Train_CV'].idxmax()]['Model']\n",
        "\n",
        "# Find the best model based on the highest mean Precision score\n",
        "best_model_precision = df_results_train_cv.loc[df_results_train_cv['Precision_Train_CV'].idxmax()]['Model']"
      ],
      "metadata": {
        "id": "Y55SkdOduAGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the best models for each metric\n",
        "print(\"Best Models by Metric:\")\n",
        "print(f\"KS: {best_model_ks}\")\n",
        "print(f\"AUC: {best_model_auc}\")\n",
        "print(f\"F1-Score: {best_model_f1}\")\n",
        "print(f\"Recall: {best_model_recall}\")\n",
        "print(f\"PRAUC: {best_model_precision}\")\n",
        "print(f\"Precision: {best_model_precision}\")"
      ],
      "metadata": {
        "id": "7v2zyZ1UuU7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating KS, AUC, PRAUC, Precision, Recall and F1 for the test data"
      ],
      "metadata": {
        "id": "r0A2dLn4ufRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the model on test data\n",
        "model_predictions = {}\n",
        "for name, model in models:\n",
        "    predictions = model.predict(X_test)\n",
        "    model_predictions[name] = predictions"
      ],
      "metadata": {
        "id": "wNcMuvG2uf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Performance Metrics for each model\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, precision_recall_curve, auc\n",
        "\n",
        "model_metrics = {}\n",
        "\n",
        "for name, predictions in model_predictions.items():\n",
        "    # AUC\n",
        "    auc_score = roc_auc_score(y_test, predictions)\n",
        "\n",
        "    # F1-score\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    # Recall\n",
        "    recall = recall_score(y_test, predictions)\n",
        "\n",
        "    # Precision\n",
        "    precision = precision_score(y_test, predictions)\n",
        "\n",
        "    # Precision-Recall Curve and PRAUC\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, predictions)\n",
        "    pr_auc = auc(recall_values, precision_values)\n",
        "\n",
        "    # KS Statistic (Custom Calculation)\n",
        "    df_results = pd.DataFrame({'y_test': y_test, 'predictions': predictions})\n",
        "    df_results['probability'] = model.predict_proba(X_test)[:, 1]\n",
        "    grouped = df_results.groupby('probability')['y_test'].agg(['sum', 'count']).reset_index()\n",
        "    grouped = grouped.rename(columns={'sum': 'events', 'count': 'non_events'})\n",
        "    grouped['cum_events'] = grouped['events'].cumsum()\n",
        "    grouped['cum_non_events'] = grouped['non_events'].cumsum()\n",
        "    grouped['ks'] = abs(grouped['cum_events'] / sum(y_test) - grouped['cum_non_events'] / (len(y_test) - sum(y_test)))\n",
        "    ks_statistic = grouped['ks'].max()\n",
        "\n",
        "    model_metrics[name] = {\n",
        "        'AUC': auc_score,\n",
        "        'F1-Score': f1,\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'PRAUC': pr_auc,\n",
        "        'KS': ks_statistic\n",
        "    }\n",
        "\n",
        "print(\"Model Performance Metrics:\")\n",
        "for name, metrics in model_metrics.items():\n",
        "    print(f\"Model: {name}\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value}\")"
      ],
      "metadata": {
        "id": "pnjnjODEujGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Organising the performance into Dataframe\n",
        "# Create a DataFrame from the model_metrics dictionary\n",
        "results_df_test= pd.DataFrame(model_metrics).T"
      ],
      "metadata": {
        "id": "1Bf9Ei3WuoUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the index column to 'Model'\n",
        "results_df_test.index.name = 'Model'"
      ],
      "metadata": {
        "id": "YwDcRtTeuqVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the tabular results\n",
        "print(results_df_test)"
      ],
      "metadata": {
        "id": "D_QYM81Iur3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting Data to excel\n",
        "# Export the DataFrame to an Excel file\n",
        "results_df_test.to_excel('/content/drive/MyDrive/GSS_Data/Fraud_test.xlsx')"
      ],
      "metadata": {
        "id": "KxZ5q8QYutSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Create a TreeExplainer from the Random Forest model\n",
        "explainer = shap.Explainer(clf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot SHAP summary plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3MeaCCGvGPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, X_test, class_inds=\"original\", class_names=model.classes_)"
      ],
      "metadata": {
        "id": "_JLkTyFTvtnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values[1], X_test)"
      ],
      "metadata": {
        "id": "i9n_Z3yevv7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install initjs()"
      ],
      "metadata": {
        "id": "8GBjgdfEvyeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Assuming you have shap_values calculated for all instances\n",
        "# You can select shap_values for a specific instance by its index\n",
        "instance_index = 10  # Replace with the index of the instance you want to plot\n",
        "\n",
        "# Example shap_values array (replace this with your actual shap_values)\n",
        "shap_values = explainer(X_test)  # Assuming you have shap values calculated"
      ],
      "metadata": {
        "id": "wa2HQlLnv306"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Water Fall plot\n",
        "\n",
        "explainer =explainer  # Your SHAP explainer\n",
        "x_test =X_test # Your test data\n",
        "\n",
        "# Get SHAP values for the given test data\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Create an Explanation object using the SHAP values, base values, and feature names\n",
        "exp = shap.Explanation(\n",
        "    values=shap_values[:, :, 1],  # Replace [0] with the index of the output you want to explain\n",
        "    base_values=shap_values.base_values[:, 1],  # Replace [0] with the index of the output you want to explain\n",
        "    data=x_test,\n",
        "    feature_names=x_test.columns\n",
        ")\n",
        "\n",
        "# Generate the waterfall plot for the selected output (index [0] in this example)\n",
        "shap.waterfall_plot(exp[20])  # Replace [0] with the index of the output you want to explain\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A3zJWPtYv-PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the partial dependence plot\n",
        "shap.plots.partial_dependence(\n",
        "    'age', final_rf_model.predict, X_test,\n",
        "    ice=False, model_expected_value=True, feature_expected_value=True\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hnkzHK18wBLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eGm9x7iw8p7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}